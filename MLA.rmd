---
title: "Prediction of Activity Levels"
author: "S. Schwarzbek"
date: "20 March 2015"
output:
  html_document: default
  pdf_document:
    fig_height: 3
---
With so many data collection points  
>...it is now possible to collect a large amount of data about personal activity relatively inexpensively. ... In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

```{r dataLoad}
library(caret)
PMLfull <- read.table(
  "C:\\Users\\steve\\Desktop\\Coursera\\MachLearn\\pml-training.csv",
  header = TRUE, sep = ",", na.strings = "#DIV/0!" )
PMLuseful <- PMLfull[, c(2, 6:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:160 )]
PMLuseful$classe <- as.factor(PMLuseful$classe)
PMLuseful$user_name <- as.factor(PMLuseful$user_name)
```

###Data 
>The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
>https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

>The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment._

###Variable Inspection  
We've reduced the variable kept in the fits for modeling. Easiest to jettison are those with many NA's. After a few views, the direct measurements (yaw, pitch, roll, on the belt, forearm, and the dumbell itself) are easiest to define and most predictive.

###Analysis  
RandomForest is thorough,  
```{r RandomForestReference}
#two.mdl <-train(classe ~ ., PMLuseful[, -c(2,3)], method = "rf")
two.mdl <- readRDS("RandomForestResults")
varImp(two.mdl)
predict(two.mdl, PMLtest[, -c(2,3)])
```  
but takes too long ( `r round(two.mdl$times$everything[3]/3600, 2)` hours elapsed on  my device) for writing up reports. We've reloaded a saved version for display.  
Our expected error, estimated fom outof bag assessment in training is 
```{r rf_summary}
two.mdl$results 
```

Sometimes, the simplest fit is the best. A five group k-means nearest neighbor classification (knn, n=5) generates a straightforward assessment, with some sense of "near misses."

```{r Knn_the_simple_way}
knn.mdl <- knn3(classe ~ ., data = PML, k=5)
knn.pred <- predict(knn.mdl, PMLtest[, -c(2,3)])
knn.pred
```

We pick out the favorite from this 5nn fit, and compare to the Random Forest fit:  
B, A, B, A, A, E, D, B, A, A, B, C, B, A, E, E, A, B, B, B  
B, A, B, A, A, E, D, B, A, A, B, C, C, A, E, E, A, B, B, B  

And find one element, 13, that has a disagreement. The 5nn fit assigned p=0.6 that it is B, p(D)= 0.4. RF pegged it as "C." The entry is ambiguous. For timeliness, the nearest neighbor algorithm comes to a conclusion quick, and shows where the assignment is suspect in a straightforward way.

Note: For commercial use, the Random Forest algorithm  should be purchased from Salford Systems:  
http://www.salford-systems.com/products/randomforests  
See license termes in the package.

